{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for psychologists: a gentle introduction\n",
    "\n",
    "This is a tutorial on machine learning. The title says it's for psychologists, but all that really means is that (a) most of the examples involve psychology data, (b) there's almost no math, and (c) I assume that the reader has some familiarity with the kinds of inferential statistical methods commonly used in psychology. For the most part, the tutorial should be equally applicable to readers coming from other social and biomedical fields. On the rare occasion I make points relevant primarily to psychologists, I highlight that fact up front, so that other readers can shrug in disgust and skip the offending section.\n",
    "\n",
    "### Overview and organization\n",
    "This tutorial is organized into several Jupyter notebooks. A Jupyter notebook is a web application that lets you combine text (or HTML), code, figures, equations, interactive widgets, and more, all in one magical document. There are currently 6 notebooks in total; each one provides an introductory look at a different topic in machine learning. The coverage is by no means exhaustive; the goal is simply to provide a high-level overview of a number of key machine learning concepts and practices.\n",
    "\n",
    "This tutorial borrows liberally from a number of others—most notably, from Jake Vanderplas's excellent [scikit-learn tutorial](https://github.com/jakevdp/sklearn_tutorial). The main difference is that the present tutorial is meant as a gentle conceptual overview of machine learning, and not as an introduction to how machine learning is done in Python or scikit-learn. This means that, in places, the code is likely to be opaque to readers who don't have much prior programming experience (either with Python in particular, or with other languages). That's okay. While I've tried to comment the code liberally, the text will mostly treat the code cells as blackboxes: I'll explain what goes in (and why), and what comes out (and why), but if you ignore the code in favor of the text and figures, you will hopefully still come out with a basic grasp on core machine learning principles.\n",
    "\n",
    "That said, one benefit of the Jupyter notebook format is that it's easy to interact with the code. To facilitate this, I've tried to put key parameters that you're encouraged to modify in constants (you'll recognize them by the fact that the variable names are all `UPPERCASE`) at the top of each cell. When you see something like `N = 30`, feel free to change it to `N = 300` and re-run the cell to see what effect it has on the generated plots or results.\n",
    "\n",
    "While most of the code is included in the tutorial notebooks themselves, in a few cases where the code is particularly long, I've abstracted it into helper functions in the `support/` folder. This is to maximize clarity, but you can always go and take a look if you want to see the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "All of the code in this tutorial is written in Python. There is nothing intrinsically special about Python in the machine learning context; in principle, all of the examples and simulations in these notebooks could have been written in other languages (R, Matlab, etc.). Indeed, there are plenty of machine learning tutorials out there written in other languages.\n",
    "\n",
    "That said, Python does have a number of practical advantages over other languages. Chief among these is the fact that it's currently by far the most widely used language in the data science and machine learning community. This means there are exceptional tools written in Python for virtually every domain of machine learning. Exhibit A is the `scikit-learn` package for machine learning, which we'll rely on heavily throughout this tutorial. Scikit-learn is the world's most widely used machine learning, and some of the reasons for its popularity will hopefully soon become clear. Scikit-learn is itself built on the [numpy](https://www.numpy.org) numerical computing library, which we'll also use fairly regular.\n",
    "\n",
    "### Data\n",
    "Rather than relying strictly on simulated data and/or the infamous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), we'll use data that should be of interest to at least some psychologists—namely, personality questionnaire data from [Johnson (2014)](http://www.personal.psu.edu/faculty/j/5/j5j/papers/JRP2014.pdf). The dataset consists of internet-based responses from over 300,000 people to a 300-item personality questionnaire—the IPIP representation of the NEO-PI-R. The NEO-PI-R is one of the most widely used measures of the Big Five. You've probably heard of the Big Five at least indirectly even if you're not a personality psychologist. If you've ever heard someone mumble something like \"no I don't want to go out right now; don't you know I'm an introvert?\", you're in the right ballpark.\n",
    "\n",
    "The citation for the data, in case you end up wanting to use it for anything else, is:\n",
    "\n",
    "> Johnson, J. A. (2014). Measuring thirty facets of the five factor model with a 120-item public domain inventory: Development of the IPIP-NEO-120. Journal of Research in Personality, 51, 78-89.\n",
    "\n",
    "The original data is available in SPSS format [here](https://osf.io/wxvth/files/). But because SPSS is evil, I've taken the small liberty of bundling this repo with a plaintext, tab-delimited version of the file. This will make our life much easier when reading the data in. I've also taken the somewhat bigger liberty of dropping all rows that contain at least one missing response. Normally, listwise deletion isn't an ideal thing to do, but our goal here is to introduce core machine learning principles, not to draw generalizable inferences about personality. So we're going to opt for expediency.\n",
    "\n",
    "With that out of the way, let's read in and look at the data. It's a pretty big dataset (~145,000 rows), so expect this to take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEC</th>\n",
       "      <th>MIN</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>...</th>\n",
       "      <th>Self-Discipline</th>\n",
       "      <th>Self-Efficacy</th>\n",
       "      <th>Sympathy</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Vulnerability</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>UK</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>3.566667</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.283333</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>3.983333</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>2.716667</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.566667</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CASE  SEX  AGE  SEC  MIN  HOUR  DATE  MONTH  YEAR    COUNTRY  ...  \\\n",
       "0     4    2   36   44   11    10    31      0   101        USA  ...   \n",
       "1     6    1   17   28   23    10    31      0   101         UK  ...   \n",
       "2     9    2   28    0   30    10    31      0   101    Denmark  ...   \n",
       "3    11    1   17   36   36    10    31      0   101  Singapore  ...   \n",
       "4    15    2   48   34   49    10    31      0   101        USA  ...   \n",
       "\n",
       "   Self-Discipline  Self-Efficacy  Sympathy  Trust  Vulnerability  \\\n",
       "0              4.2            5.0       4.0    4.1            1.1   \n",
       "1              2.2            3.7       2.8    3.5            2.5   \n",
       "2              2.7            4.2       4.0    4.7            1.9   \n",
       "3              3.6            3.0       4.2    4.2            3.3   \n",
       "4              3.9            4.0       4.4    3.1            3.2   \n",
       "\n",
       "   Neuroticism  Extraversion  Openness  Agreeableness  Conscientiousness  \n",
       "0     1.483333      3.850000  4.300000       4.400000               4.48  \n",
       "1     2.566667      3.566667  3.233333       3.000000               3.18  \n",
       "2     2.283333      2.983333  4.350000       3.983333               3.32  \n",
       "3     3.250000      2.416667  2.716667       4.200000               3.70  \n",
       "4     3.416667      3.066667  3.566667       3.900000               4.20  \n",
       "\n",
       "[5 rows x 345 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We begin by importing the pandas package, which we'll use extensively\n",
    "# for data manipulation. In future sections, we'll put the core imports\n",
    "# at the top of the notebook, which is the convention in Python.\n",
    "import pandas as pd\n",
    "\n",
    "# read_csv is a workhorse function that can read almost any kind of\n",
    "# plain-text format. The returned object is a pandas DataFrame.\n",
    "all_data = pd.read_csv('data/Johnson-2014-IPIP-300.tsv.gz', sep='\\t')\n",
    "\n",
    "# head() display the first few rows of the dataset.\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data consist of 310 columns: 10 demographic and contextual variables (sex, age, country, etc.) and 300 individual items, where each response is on a 5-point likert scale (reverse-keyed items have already been reversed). For example, here's the response distribution for the 100th item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### tells the jupyter notebook to display plots in-line\n",
    "%matplotlib inline\n",
    "\n",
    "all_data['I100'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned-up version of the data I've provided here supplements the demographic and item variables with 30 facet score and 5 domains. The 30 facets are supposed to represent relatively narrow personality traits; each is scored by averaging 10 of the 300 items. The facet scores, in turn, can be averaged (in groups of 6) to form the canonical Big Five constructs—Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness.\n",
    "\n",
    "#### Easier data retrieval\n",
    "A lot of our analyses are going to make use of different subsets of the overall dataset. In particular, we'll frequently want to (a) sample subsets of rows, and (b) compare different sets of predictors (e.g., the 5 domains, 30 facets, and 300 individual items). To save ourselves from having to write the same subsetting code over and over, let's abstract it into a small helper function (in the other notebooks, we'll import the same function from our `support/` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, *args, n=None):\n",
    "    ''' Return specified features or feature groups for a subsample.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): DataFrame to subsample.\n",
    "        args (list): Positional args containing the names of\n",
    "            variables/groups to return. Each element must be one of\n",
    "            'domains', 'facets', 'items', or the name of an existing\n",
    "            column.\n",
    "        n (int): Number of cases to return. If None, keeps all rows.\n",
    "\n",
    "    Returns: A list of pandasDataFrames and/or Series, in the same\n",
    "        order as *args.\n",
    "    '''\n",
    "\n",
    "    if n is not None:\n",
    "        data = data.sample(n)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name in args:\n",
    "        if name == 'domains':\n",
    "            results.append(data.iloc[:, -5:])\n",
    "        elif name == 'facets':\n",
    "            results.append(data.iloc[:, -35:-5])\n",
    "        elif name == 'items':\n",
    "            results.append(data.iloc[:, -335:-35])\n",
    "        else:\n",
    "            results.append(data[name])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to get just the Big Five domain scores for 1000 random rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84200</th>\n",
       "      <td>2.916667</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120225</th>\n",
       "      <td>2.733333</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>3.966667</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128899</th>\n",
       "      <td>3.216667</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>3.783333</td>\n",
       "      <td>4.016667</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100781</th>\n",
       "      <td>2.800000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80184</th>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.516667</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Neuroticism  Extraversion  Openness  Agreeableness  Conscientiousness\n",
       "84200      2.916667      3.500000  4.550000       3.500000               3.46\n",
       "120225     2.733333      3.133333  3.150000       3.966667               4.46\n",
       "128899     3.216667      2.750000  3.783333       4.016667               3.48\n",
       "100781     2.800000      3.150000  3.383333       3.633333               3.02\n",
       "80184      2.666667      3.666667  3.516667       3.650000               3.62"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that get_features() always returns a list, so when\n",
    "# there's only a single requested object, we index with [0].\n",
    "domains = get_features(all_data, 'domains', n=1000)[0]\n",
    "\n",
    "# Display first few rows\n",
    "domains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some features! If you're wondering what the hell a \"feature\" is, fear not—it's just machine learning-speak for \"variable\".\n",
    "\n",
    "#### A suitable outcome\n",
    "For most of this tutorial, we'll be using personality scores as the features in our predictive models. Of course, we also need something *to* predict. Fortunately, the Johnson (2014) dataset includes a number of basic demographic variables. We'll mostly focus on age, which is a nice target for prediction, because we might intuitively expect there to be systematic relationships between people's age and their self-reported personality scores. Let's confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neuroticism         -0.12\n",
       "Extraversion        -0.12\n",
       "Openness             0.01\n",
       "Agreeableness        0.16\n",
       "Conscientiousness    0.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get domain scores and age and assign to variables\n",
    "domains, age = get_features(all_data, 'domains', 'AGE')\n",
    "\n",
    "# Pearson correlation between columns in domains and age\n",
    "domains.corrwith(age).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four out of 5 of the Big Five dimensions are non-trivially correlated with age, and the relationship with conscientiousness is particularly robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
